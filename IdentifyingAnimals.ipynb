{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3070 Ti\n",
      "VRAM: 8.0GB\n",
      "üéØ Specific model: convnext_large.fb_in22k_ft_in1k\n",
      "üìè Input size: 224√ó224\n",
      "üìÇ Loading data...\n",
      "Found 23 classes, 13711 images\n",
      "Train set: 11654 samples\n",
      "Validation set: 2057 samples\n",
      "üîç Loading model: convnext_large.fb_in22k_ft_in1k\n",
      "‚úÖ Model loaded successfully: convnext_large.fb_in22k_ft_in1k\n",
      "üìà Number of parameters: 196,265,687\n",
      "\n",
      "üöÄ Start ConvNeXt-specific training...\n",
      "Using model: convnext_large.fb_in22k_ft_in1k\n",
      "\n",
      "Epoch 1/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [02:26<00:00,  4.98it/s, Loss=0.1409, Acc=91.81%, LR=5.00e-04]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:16<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3347, Train accuracy: 0.9181\n",
      "Val loss: 0.2297, Val accuracy: 0.9397\n",
      "üéâ New best accuracy: 0.9397 (Epoch 1)\n",
      "Learning rate: 9.76e-06\n",
      "\n",
      "Epoch 2/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [02:23<00:00,  5.08it/s, Loss=0.0098, Acc=97.19%, LR=4.88e-04]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:13<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1097, Train accuracy: 0.9719\n",
      "Val loss: 0.2190, Val accuracy: 0.9451\n",
      "üéâ New best accuracy: 0.9451 (Epoch 2)\n",
      "Learning rate: 9.05e-06\n",
      "\n",
      "Epoch 3/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [02:24<00:00,  5.06it/s, Loss=0.0268, Acc=98.66%, LR=4.52e-04] \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:13<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0573, Train accuracy: 0.9866\n",
      "Val loss: 0.2386, Val accuracy: 0.9470\n",
      "üéâ New best accuracy: 0.9470 (Epoch 3)\n",
      "Learning rate: 7.94e-06\n",
      "\n",
      "Epoch 4/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [02:26<00:00,  4.97it/s, Loss=0.0013, Acc=99.06%, LR=3.97e-04] \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:13<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0366, Train accuracy: 0.9906\n",
      "Val loss: 0.2545, Val accuracy: 0.9441\n",
      "Learning rate: 6.55e-06\n",
      "\n",
      "Epoch 5/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [02:25<00:00,  5.01it/s, Loss=0.0003, Acc=99.34%, LR=3.27e-04] \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:13<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0249, Train accuracy: 0.9934\n",
      "Val loss: 0.2678, Val accuracy: 0.9451\n",
      "Learning rate: 5.00e-06\n",
      "\n",
      "Epoch 6/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [02:26<00:00,  4.97it/s, Loss=0.0010, Acc=99.41%, LR=2.50e-04] \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:13<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0207, Train accuracy: 0.9941\n",
      "Val loss: 0.2580, Val accuracy: 0.9494\n",
      "üéâ New best accuracy: 0.9494 (Epoch 6)\n",
      "Learning rate: 3.45e-06\n",
      "\n",
      "Epoch 7/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [02:26<00:00,  4.98it/s, Loss=0.0033, Acc=99.43%, LR=1.73e-04] \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:13<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0168, Train accuracy: 0.9943\n",
      "Val loss: 0.2493, Val accuracy: 0.9499\n",
      "üéâ New best accuracy: 0.9499 (Epoch 7)\n",
      "Learning rate: 2.06e-06\n",
      "\n",
      "Epoch 8/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [02:26<00:00,  4.96it/s, Loss=0.0022, Acc=99.45%, LR=1.03e-04] \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:13<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0118, Train accuracy: 0.9945\n",
      "Val loss: 0.2518, Val accuracy: 0.9504\n",
      "üéâ New best accuracy: 0.9504 (Epoch 8)\n",
      "Learning rate: 9.55e-07\n",
      "\n",
      "Epoch 9/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [02:26<00:00,  4.99it/s, Loss=0.0006, Acc=99.51%, LR=4.77e-05] \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:13<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0091, Train accuracy: 0.9951\n",
      "Val loss: 0.2593, Val accuracy: 0.9494\n",
      "Learning rate: 2.45e-07\n",
      "\n",
      "Epoch 10/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [02:26<00:00,  4.96it/s, Loss=0.0004, Acc=99.55%, LR=1.22e-05] \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:13<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0073, Train accuracy: 0.9955\n",
      "Val loss: 0.2567, Val accuracy: 0.9490\n",
      "Learning rate: 0.00e+00\n",
      "\n",
      "üèÜ Training complete! Best validation accuracy: 0.9504 (Epoch 8)\n",
      "\n",
      "üèÜ Final validation accuracy: 0.9504\n",
      "üìÇ Loaded best model, accuracy: 0.9504\n",
      "üìÖ Best epoch: 8\n",
      "üîÆ Start ConvNeXt-specific TTA prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Predict: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Prediction stats:\n",
      "Samples: 5\n",
      "Mean confidence: 0.9988\n",
      "Min confidence: 0.9956\n",
      "Max confidence: 1.0000\n",
      "High-confidence samples (>0.8): 5/5 (100.0%)\n",
      "Medium-confidence samples (0.5-0.8): 0/5 (0.0%)\n",
      "Low-confidence samples (‚â§0.5): 0/5 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# üîë Set random seed (updated to 1029)\n",
    "torch.manual_seed(1029)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/(1024**3):.1f}GB\")\n",
    "\n",
    "\n",
    "# üîÑ Update to local paths\n",
    "train_dir = r\"C:\\Users\\Aufb\\Downloads\\fish_dataset_categories\"\n",
    "test_dir  = r\"C:\\Users\\Aufb\\Desktop\\Test\"\n",
    "output_file = r\"C:\\Users\\Aufb\\Desktop\\FIT5210_Life_Prediction.csv\"\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    print(f\"‚ùå Data directory does not exist: {train_dir}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "MODEL_NAME = 'convnext_large.fb_in22k_ft_in1k'\n",
    "TARGET_SIZE = 224  \n",
    "\n",
    "print(f\"üéØ Specific model: {MODEL_NAME}\")\n",
    "print(f\"üìè Input size: {TARGET_SIZE}√ó{TARGET_SIZE}\")\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(int(TARGET_SIZE * 1.2)),\n",
    "    transforms.RandomResizedCrop(TARGET_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(int(TARGET_SIZE * 1.1)),\n",
    "    transforms.CenterCrop(TARGET_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "print(\"üìÇ Loading data...\")\n",
    "full_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "print(f\"Found {len(full_dataset.classes)} classes, {len(full_dataset)} images\")\n",
    "\n",
    "\n",
    "train_size = int(0.85 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "val_dataset.dataset.transform = test_transform\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16 if torch.cuda.is_available() else 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train set: {len(train_dataset)} samples\")\n",
    "print(f\"Validation set: {len(val_dataset)} samples\")\n",
    "\n",
    "\n",
    "print(f\"üîç Loading model: {MODEL_NAME}\")\n",
    "try:\n",
    "    model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=len(full_dataset.classes))\n",
    "    model = model.to(device)\n",
    "    print(f\"‚úÖ Model loaded successfully: {MODEL_NAME}\")\n",
    "    \n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"üìà Number of parameters: {total_params:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.stem.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.stages.parameters(), 'lr': 2e-5},\n",
    "    {'params': model.head.parameters(), 'lr': 5e-4}\n",
    "], weight_decay=0.05)\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "def train_convnext():\n",
    "    print(f\"\\nüöÄ Start ConvNeXt-specific training...\")\n",
    "    print(f\"Using model: {MODEL_NAME}\")\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    \n",
    "    for epoch in range(10):\n",
    "        print(f'\\nEpoch {epoch+1}/10')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            \n",
    "            current_lr = optimizer.param_groups[2]['lr']\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}', \n",
    "                'Acc': f'{100.*train_correct/train_total:.2f}%',\n",
    "                'LR': f'{current_lr:.2e}'\n",
    "            })\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in tqdm(val_loader, desc='Validation'):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                \n",
    "                if scaler:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output = model(data)\n",
    "                else:\n",
    "                    output = model(data)\n",
    "                \n",
    "                val_loss += criterion(output, target).item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f'Train loss: {avg_train_loss:.4f}, Train accuracy: {train_acc:.4f}')\n",
    "        print(f'Val loss: {avg_val_loss:.4f}, Val accuracy: {val_acc:.4f}')\n",
    "        \n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "                'class_names': full_dataset.classes,\n",
    "                'model_name': MODEL_NAME,\n",
    "                'target_size': TARGET_SIZE,\n",
    "                'epoch': epoch + 1\n",
    "            }, 'convnext_best_model.pth')\n",
    "            print(f'üéâ New best accuracy: {best_acc:.4f} (Epoch {epoch+1})')\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f'Learning rate: {scheduler.get_last_lr()[0]:.2e}')\n",
    "    \n",
    "    print(f'\\nüèÜ Training complete! Best validation accuracy: {best_acc:.4f} (Epoch {best_epoch})')\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "final_acc = train_convnext()\n",
    "print(f\"\\nüèÜ Final validation accuracy: {final_acc:.4f}\")\n",
    "\n",
    "\n",
    "checkpoint = torch.load('convnext_best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "class_names = checkpoint['class_names']\n",
    "print(f\"üìÇ Loaded best model, accuracy: {checkpoint['best_acc']:.4f}\")\n",
    "print(f\"üìÖ Best epoch: {checkpoint['epoch']}\")\n",
    "\n",
    "\n",
    "def convnext_tta_predict():\n",
    "    if not os.path.exists(test_dir):\n",
    "        print(f\"‚ùå Test directory does not exist: {test_dir}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(\"üîÆ Start ConvNeXt-specific TTA prediction...\")\n",
    "    \n",
    "    \n",
    "    tta_transforms = [\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(int(TARGET_SIZE * 1.1)),\n",
    "            transforms.CenterCrop(TARGET_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(int(TARGET_SIZE * 1.1)),\n",
    "            transforms.CenterCrop(TARGET_SIZE),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(int(TARGET_SIZE * 1.2)),\n",
    "            transforms.CenterCrop(TARGET_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(int(TARGET_SIZE * 1.15)),\n",
    "            transforms.CenterCrop(TARGET_SIZE),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        from PIL import Image\n",
    "        \n",
    "        \n",
    "        test_images = []\n",
    "        for root, _, files in os.walk(test_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    test_images.append(os.path.join(root, file))\n",
    "        \n",
    "        test_images.sort()  \n",
    "        \n",
    "        for img_path in tqdm(test_images, desc=\"TTA Predict\"):\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            \n",
    "            tta_outputs = []\n",
    "            for tta_transform in tta_transforms:\n",
    "                img_tensor = tta_transform(img).unsqueeze(0).to(device)\n",
    "                \n",
    "                if scaler:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output = model(img_tensor)\n",
    "                else:\n",
    "                    output = model(img_tensor)\n",
    "                \n",
    "                tta_outputs.append(torch.softmax(output, dim=1))\n",
    "            \n",
    "            \n",
    "            avg_output = torch.stack(tta_outputs).mean(dim=0)\n",
    "            confidence, prediction = torch.max(avg_output, dim=1)\n",
    "            \n",
    "            predictions.append(prediction.item())\n",
    "            confidences.append(confidence.item())\n",
    "    \n",
    "    return predictions, confidences\n",
    "\n",
    "\n",
    "test_predictions, test_confidences = convnext_tta_predict()\n",
    "\n",
    "if test_predictions:\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': range(len(test_predictions)),\n",
    "        'Label': [class_names[pred] for pred in test_predictions]\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv(output_file, index=False)  # ‚¨ÖÔ∏è Save to the specified output path\n",
    "    \n",
    "    \n",
    "    detailed_df = pd.DataFrame({\n",
    "        'ID': range(len(test_predictions)),\n",
    "        'Label': [class_names[pred] for pred in test_predictions],\n",
    "        'Confidence': test_confidences\n",
    "    })\n",
    "    detailed_df.to_csv(output_file.replace(\".csv\", \"_detailed.csv\"), index=False)\n",
    "    \n",
    "    print(f\"\\nüìä Prediction stats:\")\n",
    "    print(f\"Samples: {len(test_predictions)}\")\n",
    "    print(f\"Mean confidence: {np.mean(test_confidences):.4f}\")\n",
    "    print(f\"Min confidence: {np.min(test_confidences):.4f}\")\n",
    "    print(f\"Max confidence: {np.max(test_confidences):.4f}\")\n",
    "    \n",
    "    \n",
    "    high_conf_count = sum(1 for c in test_confidences if c > 0.8)\n",
    "    medium_conf_count = sum(1 for c in test_confidences if 0.5 < c <= 0.8)\n",
    "    low_conf_count = sum(1 for c in test_confidences if c <= 0.5)\n",
    "    \n",
    "    print(f\"High-confidence samples (>0.8): {high_conf_count}/{len(test_confidences)} ({100*high_conf_count/len(test_confidences):.1f}%)\")\n",
    "    print(f\"Medium-confidence samples (0.5-0.8): {medium_conf_count}/{len(test_confidences)} ({100*medium_conf_count/len(test_confidences):.1f}%)\")\n",
    "    print(f\"Low-confidence samples (‚â§0.5): {low_conf_count}/{len(test_confidences)} ({100*low_conf_count/len(test_confidences):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
